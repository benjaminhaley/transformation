---
output: 
  html_document:
    theme: journal
---

### Learning the rules of image transformation
*Benjamin Haley 2014*  
[rpub][rpubs] - [code][github] - [blog][blogger]

**How a computer can learn the rules of rotation, reflection, scaling, translation, and many other transformations that images can undergo.**

__________________________________________________________________


#### We recognize images despite transformations

As your eyes move across this sentence the image hitting your retina is constantly shifting.  Yet you hardly notice.  One reason you do not, is because your brain recognizes letters regardless of their position in your field of view.

Consider the following image.  Look first at the blue dot and then at the red.  Notice that the number '2' between them is recognizable regardless of your focus.  This, despite the fact that the image is falling on a completely different set of neurons.

![][blue-2-red]

Images go through many such transformations. They reverse, rotate, scale, translate and distort in many ways we have no words for.  That's not to mention all the changes in lighting that can occur.  Through all of this they remain recognizable.

![][types-of-transformations]

The number of transformations can happen to an image is infinite, but that does not mean that all transformations are possible or probable.  Many never occur in the real world and our brain is cannot recognize the images after these improbable transformations.

![][impossible-transformation]

__________________________________________________________________

#### But computers are bad at learning the rules of image transformation
The rules of transformation are important for anyone who wants to teach a computer how to process images.  [The algorithms that are best at image recognition][convolution] learn a representation of the world that considers many shifts of focus, [translations][translations], as illustrated in the first example.

However, these algorithms do not *learn* that images can be translated, the way they learn to recognize digits.  Instead the laws of translation are programmed into the algorithm by the researcher.

Would it be possible to have them learn about translation without telling them explicitly?  What about the myriad other transformations that are possible?

__________________________________________________________________

#### I propose that these transformations can be learned
I propose they can, and submit the following experiment as evidence.  In it I show that we can learn that flipping an image upside down is a valid transformation, but randomly re-arranging the pixels is not.  I conjecture that this proceedure can be generalized to learn many other types of possible transformations for image and non-image data.

__________________________________________________________________

```{r global_options, include=FALSE}
# Global knitr configuration options
# This will supress warnings and make graphs a nice size.

library(knitr)
opts_chunk$set(fig.width=3, 
               fig.height=2, 
               echo=TRUE,         # Toggle this to show the code
               warning=FALSE, 
               message=FALSE,
               results='asis'
               )
```

```{r}
# Set up
# Load in a few libraries, define some helpful functions, load a small sample of [mnist] data, and convert it to a useful format.

# Libraries
library(ggplot2)
library(reshape2)
library(plyr)
library(dplyr)
library(RCurl)
library(xtable)
library(gtools)

# Functions to map pixel number to x, y coordinates (and back)
y <- function(pixel) -floor(pixel / 28) + 28
x <- function(pixel) pixel %% 28
pixel <- function(x, y) (28 - y) * 28 + x

# Compress all columns to one
concatenate_columns <- function(df) do.call("paste", df)

# Convert data 
# from one image per row to one pixel per row
elongate <- function(d=data){
  d <- d[,order(names(d))] %>%
    mutate(pattern = concatenate_columns(d),
           id = 1:nrow(d))
  d <- melt(d, 
            id.vars=c('id', 'pattern'), 
            variable.name='pixel', 
            value.name='intensity')
  d <- d %>%
    mutate(pixel = as.numeric(as.character(pixel)))
  d
}

# A minimalist theme for plotting
theme_nothing = function(...) theme(
  line=element_blank(),
  text=element_blank(),
  title=element_blank(),
  rect=element_blank(),
  legend.position="none")

# Show images
# With appropriate x, y positions and one image
# per facet
show <- function(g, modifications=theme_nothing()) {
  ggplot(g, aes(x(pixel), y(pixel), alpha=intensity)) + 
    geom_tile() + 
    facet_wrap(~ pattern + id, ncol=20) + 
    theme_nothing() +
    modifications
}

# A version of table which determines frequency
# and outputs a data.frame
tabulate_frequencies <- function(x) {
  data.frame(value=x) %>% 
    group_by(value) %>% 
    summarize(count = length(value),
              frequency = count/length(x))
}

# Determine the loglikeliood of the patterns
# in y given the patterns in x
loglikelihood <- function(x, y){
  x <- tabulate_frequencies(x)
  y <- tabulate_frequencies(y)

  d <- merge(x %>% select(-count), 
        y %>% select(-frequency))

  loglikelihood = summarize(d, loglikelihood=sum(count*log(frequency)))
  loglikelihood
}

# Determine how likely each result is after setting
# the most likely result to 1.
relative_likelihood <- function(loglikelihood) {
  exp(loglikelihood - max(loglikelihood))
}


# Data
# A sample from mnist, of handwritten digits.  Originally from Yann
# LeCunn's site [1], but actually sampled from Joseph Redmon's handy
# csv version [2].
# 
# [1]: http://yann.lecun.com/exdb/mnist/
# [2]: http://www.pjreddie.com/projects/mnist-in-csv/

data <- "https://raw.githubusercontent.com/benjaminhaley/transformation/master/mnist_tiny.csv"
data <- read.csv(textConnection(getURL(data)))

# Convert to binary
# Originally pixel intensity is 0-256, make it on or off
data <- data.frame(data > 100) + 0
names(data) <- c(0:783)
```

__________________________________________________________________

#### What data are we using?
To illustrate my point, I will use [Mnist][mnist] data, a handy collection of handwritten digits.

```{r}
g <- elongate(head(data, 3))
show(g)
```

__________________________________________________________________

#### Where to focus
We will focus on a small region of data.  Specifically, the three vertical pixels highlighted in the data below.

```{r}
# Highlight areas of interest
g <- g %>%
  mutate(of_interest = as.numeric(
    x(pixel) == 13 & 
    y(pixel) %in% c(13:15)))
show(g, geom_tile(aes(fill=of_interest)))
```

__________________________________________________________________

#### What patterns are common?
If we look at these three pixels across many images, we see that certain patterns are more common than others.  For example, there are many cases where one of the three pixels is blank but only one case where this is the middle pixel.

Clearly the patterns observed are not random ones.

```{r}
g <- elongate(data[,pixel(x=13, y=13:15)])
show(g)
```

#### Upside down, the patterns have similar frequencies.
If we flip the pixels upside down, the patterns have roughly the same frequency as before.

![][flipping_pixels_upside_down]

```{r}
g <- data[,pixel(x=13, y=13:15)]
new_order <- c(3, 2, 1)
names(g) <- names(g)[new_order]
g <- elongate(g)
show(g)
```

#### But switching the first two pixels produces very different frequencies.
If we make an improbable change, switching the first two pixels, while keeping the third in place, the patterns have a very different frequency than the prior cases.  For example the pattern where two filled in pixels surround a blank pixel is common, where it was only observed once in the previous examples.

![][switching_first_two_pixels]

```{r}
g <- data[,pixel(x=13, y=13:15)]
new_order <- c(2, 1, 3)
names(g) <- names(g)[new_order]
g <- elongate(g)
show(g)
```

#### What's going on?
What we are seeing is the difference between a probable image transformation, flipping upside down, and an improbable one, only switching the first two pixels.  After a probable transformation, the image retains the same patterns as any other image.  After an improbable transformation the image turns to static.

Notice that images in nature seldom look like static.  This is because they rotate, translate, scale, and otherwise distort in ways that are far from random.  Static images only emerge after random transformations, and so they contain patterns that rarely occur in nature.

#### How can we quantify the effect?
TODO describe this

```{r}
g <- data[,pixel(x=13, y=13:15)]
orders <- permutations(3, 3, 1:3)
g <- ldply(1:nrow(orders), function(possibility) {
  order <- orders[possibility,]
  data.frame(  
    loglikelihood = loglikelihood(concatenate_columns(g), 
                                  concatenate_columns(g[,order])),
    order = paste(order, collapse=' ')
  )
})
g <- g %>%
  mutate(relative_likelihoods = relative_likelihood(loglikelihood),
         result = round(100 * relative_likelihoods, 1)) %>%
  select(order, result)

print(xtable(g), 
      type="html",
      include.rownames = FALSE,
      html.table.attributes = getOption("xtable.html.table.attributes", "border=0"),
      witdth=)
```


```{r}

# TODO move to top
library(reshape2)

g <- data
# TODO stop sampling just some of them
values = c('0 0', '0 1', '1 0', '1 1')
default <- data.frame(value=values, 
                      count=0, 
                      frequency=0.0002)
columns <- sample(1:ncol(g), 200)
r <- ldply(columns, function(a) {
  ldply(columns, function(b) {
    if(a <= b) return(NULL)
    
    f <- tabulate_frequencies(concatenate_columns(g[,c(a,b)]))
    f <- rbind(f, default %>% filter(!(value %in% f$value)))

    data.frame(a, b, f)
  })
})

# log transform
r <- r %.%
  mutate(
    frequency = frequency - 0.0001,
    frequency = log(frequency / (1 - frequency)))

# cast
r <- dcast(r, a + b ~ value, value.var=c('frequency'))

# TODO logistic xform
# TODO play with # of clusters
r$cluster <- kmeans(r[,values], 30)$cluster
r <- r %>%
  mutate(x=x(a), xend=x(b), y=y(a), yend=y(b))

# Add cluster counts
counts <- r %>% group_by(cluster) %>% summarize(n=length(cluster))
r <- merge(r, counts)

ggplot(r, aes()) +
  geom_segment(aes(x=x, xend=xend, y=y, yend=yend,
               width=1/(((xend - x)^2 + (yend - y)^2)^0.5),
               alpha=1/n)) + 
  facet_wrap(~ cluster) +
  theme_nothing()


a = 47
b = 445



```

[blue-2-red]: http://dl.dropbox.com/u/1131693/bloodrop/Screenshot%202014-07-07%2017.01.09.png
[convolution]: https://en.wikipedia.org/wiki/Convolutional_neural_network
[translations]: https://en.wikipedia.org/wiki/Translation_(geometry)
[types-of-transformations]: http://dl.dropbox.com/u/1131693/bloodrop/Screenshot%202014-07-07%2020.36.55.png
[youtube-cat]: http://googleblog.blogspot.com/2012/06/using-large-scale-brain-simulations-for.html
[impossible-transformation]: http://dl.dropbox.com/u/1131693/bloodrop/Screenshot%202014-07-08%2022.46.27.png
[rpubs]: http://rpubs.com/benjaminhaley/imagetransformation
[github]: https://github.com/benjaminhaley/transformation
[blogger]: TODO
[mnist]: http://yann.lecun.com/exdb/mnist/
[flipping_pixels_upside_down]: http://dl.dropbox.com/u/1131693/bloodrop/Screenshot%202014-07-08%2023.03.39.png
[switching_first_two_pixels]: http://dl.dropbox.com/u/1131693/bloodrop/Screenshot%202014-07-08%2023.11.37.png

TODO: Add github readme file
TODO: spell check